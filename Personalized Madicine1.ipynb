{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3321 entries, 0 to 3320\n",
      "Data columns (total 5 columns):\n",
      "ID           3321 non-null int64\n",
      "Gene         3321 non-null object\n",
      "Variation    3321 non-null object\n",
      "Class        3321 non-null int64\n",
      "Text         3321 non-null object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 155.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3689 entries, 0 to 367\n",
      "Data columns (total 5 columns):\n",
      "ID           3689 non-null int64\n",
      "Gene         3689 non-null object\n",
      "Variation    3689 non-null object\n",
      "Class        3689 non-null int64\n",
      "Text         3689 non-null object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 172.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load training variants\n",
    "train = pd.read_csv('training_variants')\n",
    "# load training text\n",
    "train_txt_ = pd.read_csv('training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "# merge text & variants\n",
    "train = pd.merge(train, train_txt_, how='left', on='ID').fillna('')\n",
    "# clean up\n",
    "del train_txt_\n",
    "# print train data info\n",
    "display(train.info())\n",
    "\n",
    "# load test variants from stage 1\n",
    "testold_var_ = pd.read_csv('test_variants')\n",
    "# load test text from stage 1\n",
    "testold_txt_ = pd.read_csv('test_text', sep='\\|\\|', engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "# merge text & variants\n",
    "testold_ = pd.merge(testold_var_, testold_txt_, how='left', on='ID').fillna('')\n",
    "# clean up\n",
    "del testold_var_\n",
    "del testold_txt_\n",
    "\n",
    "# load stage1 solutions\n",
    "stage1sol_ = pd.read_csv('stage1_solution_filtered.csv')\n",
    "# get class\n",
    "stage1sol_['Class'] = pd.to_numeric(stage1sol_.drop('ID', axis=1).idxmax(axis=1).str[5:]).fillna(0).astype(np.int64)\n",
    "# drop records from testold_ if they are not in stage1sol_\n",
    "testold_ = testold_[testold_.index.isin(stage1sol_['ID'])]\n",
    "# merge class to testold_ from stage1sol_\n",
    "newtraindata_ = testold_.merge(stage1sol_[['ID', 'Class']], on='ID', how='left')\n",
    "# reindex columns\n",
    "newtraindata_ = newtraindata_.reindex_axis(['ID','Gene','Variation','Class','Text'], axis=1)\n",
    "# clean up\n",
    "del stage1sol_\n",
    "del testold_\n",
    "\n",
    "# append new train data\n",
    "train = train.append(newtraindata_)\n",
    "# clean up\n",
    "del newtraindata_\n",
    "\n",
    "# print train data info\n",
    "display(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "word2vec not loaded!\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec = None\n",
    "# make sure you load this on your local env and uncomment the line\n",
    "# word2vec = KeyedVectors.load_word2vec_format('PubMed-and-PMC-w2v.bin', binary=True)\n",
    "if (word2vec == None):\n",
    "    print(\"word2vec not loaded!\")\n",
    "else:\n",
    "    print(\"Found {} word vectors of word2vec\".format(len(word2vec.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expand records to sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Gene</th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>0</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>0</td>\n",
       "      <td>Altogether, our results reveal an additional r...</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>0</td>\n",
       "      <td>We show that a recombinant CDK10/cyclin M hete...</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>0</td>\n",
       "      <td>An interaction phenotype was also observed bet...</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>0</td>\n",
       "      <td>Black boxes indicate internal deletions. The r...</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class    Gene  ID                                               Text  \\\n",
       "0      1  FAM58A   0  Cyclin-dependent kinases (CDKs) regulate a var...   \n",
       "1      1  FAM58A   0  Altogether, our results reveal an additional r...   \n",
       "2      1  FAM58A   0  We show that a recombinant CDK10/cyclin M hete...   \n",
       "3      1  FAM58A   0  An interaction phenotype was also observed bet...   \n",
       "4      1  FAM58A   0  Black boxes indicate internal deletions. The r...   \n",
       "\n",
       "              Variation  \n",
       "0  Truncating Mutations  \n",
       "1  Truncating Mutations  \n",
       "2  Truncating Mutations  \n",
       "3  Truncating Mutations  \n",
       "4  Truncating Mutations  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expanded from 3689 to 108046\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Create a function called \"chunks\" with two arguments, l and n:\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i+n]\n",
    "\n",
    "print('Expand records to sentences.')\n",
    "# increase maxnumberofsentecs on local env to 400\n",
    "maxnumberofsentences = 400\n",
    "# increase splitbysenteces on local env to 10\n",
    "splitbysentences = 10\n",
    "# temp dict for new train set\n",
    "tmpdf_ = {'Text': [], 'Class': [], 'ID': [], 'Gene': [], 'Variation': []}\n",
    "for index, row in train.iterrows():\n",
    "    # get sentences nltk\n",
    "    sent_tokenize_list = nltk.sent_tokenize(row['Text'])\n",
    "    # truncate sentences to last maxnumberofsentences (most important informations are at the end of text)\n",
    "    if (len(sent_tokenize_list) > maxnumberofsentences):\n",
    "        sent_tokenize_list = sent_tokenize_list[len(sent_tokenize_list)-maxnumberofsentences:]\n",
    "    # split sentences to batch\n",
    "    sent_chunk = list(chunks(sent_tokenize_list, splitbysentences))\n",
    "    for chunk in sent_chunk:\n",
    "        # join sentences in text\n",
    "        tmpdf_['Text'].append(\" \".join(chunk))\n",
    "        # assign class\n",
    "        tmpdf_['Class'].append(row['Class'])\n",
    "        # assign ID\n",
    "        tmpdf_['ID'].append(row['ID'])\n",
    "        # assign Gene\n",
    "        tmpdf_['Gene'].append(row['Gene'])\n",
    "        # assign Variation\n",
    "        tmpdf_['Variation'].append(row['Variation'])\n",
    "# create new train set from temp dict\n",
    "origtrainlen = len(train)\n",
    "train = pd.DataFrame(tmpdf_)\n",
    "# clean up\n",
    "del tmpdf_\n",
    "# display head\n",
    "display(train.head())\n",
    "# display \n",
    "print('expanded from {} to {}'.format(origtrainlen,len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\divanshu\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 149629 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# max top words, increase on local env to 100000\n",
    "num_words = 100000\n",
    "# max sequence length, increase on local env to 500\n",
    "sequencelength = 500\n",
    "# init tokenizer\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "# fit tokenizer\n",
    "tokenizer.fit_on_texts(train['Text'])\n",
    "# get sequences\n",
    "X = tokenizer.texts_to_sequences(train['Text'])\n",
    "# unique words in text\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found {} unique tokens.\".format(len(word_index)))\n",
    "# pad sequences\n",
    "X = pad_sequences(X, maxlen=sequencelength)\n",
    "\n",
    "embedding_matrix = None\n",
    "if (word2vec != None):\n",
    "    # out of vocabulary words > use this to do text analysis\n",
    "    oov_words = []\n",
    "    # prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_words+1, 200)) #200 = word2vec dim\n",
    "    for word, i in word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        if word in word2vec.vocab:\n",
    "            # embedd from word2vec\n",
    "            embedding_matrix[i] = word2vec.word_vec(word)\n",
    "        else:\n",
    "            # add to out of vocabulary\n",
    "            oov_words.append(word)\n",
    "    print('Preparing embedding matrix done. out-of-vocabulary rate (OOV): {} ({})'.format(len(oov_words)/float(len(word_index)),len(oov_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86436 samples, validate on 21610 samples\n",
      "Epoch 1/5\n",
      "86436/86436 [==============================] - 4355s 50ms/step - loss: 1.1375 - categorical_crossentropy: 1.1375 - val_loss: 1.4105 - val_categorical_crossentropy: 1.4105\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.41048, saving model to model\n",
      "Epoch 2/5\n",
      "86436/86436 [==============================] - 4187s 48ms/step - loss: 0.8042 - categorical_crossentropy: 0.8042 - val_loss: 1.5281 - val_categorical_crossentropy: 1.5281\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.41048\n",
      "Epoch 3/5\n",
      "86436/86436 [==============================] - 4022s 47ms/step - loss: 0.7078 - categorical_crossentropy: 0.7078 - val_loss: 1.6524 - val_categorical_crossentropy: 1.6524\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.41048\n",
      "Epoch 4/5\n",
      "86436/86436 [==============================] - 4027s 47ms/step - loss: 0.6562 - categorical_crossentropy: 0.6562 - val_loss: 1.7068 - val_categorical_crossentropy: 1.7068\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.41048\n",
      "Epoch 5/5\n",
      "86436/86436 [==============================] - 4251s 49ms/step - loss: 0.6177 - categorical_crossentropy: 0.6177 - val_loss: 1.8316 - val_categorical_crossentropy: 1.8316\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.41048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x225b6b619e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "embed_dim = 200 #same as word2vec dim\n",
    "\n",
    "model_filename = 'model'\n",
    "\n",
    "# prepare Y values\n",
    "Y = train['Class'].values-1\n",
    "# get weights for unevenly distributed dataset \n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(Y), Y)\n",
    "# one hot\n",
    "Y = keras.utils.to_categorical(Y)\n",
    "# batch size increase on local env\n",
    "batch_size = 30\n",
    "# epochs increase on local env\n",
    "epochs = 5\n",
    "# Model saving callback\n",
    "ckpt_callback = keras.callbacks.ModelCheckpoint(model_filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "# input layer\n",
    "input1 = keras.layers.Input(shape=(sequencelength,))\n",
    "# embedding layer\n",
    "if (embedding_matrix == None):\n",
    "    # word2vec was not loaded. use fallback method\n",
    "    embedding = keras.layers.Embedding(num_words+1, embed_dim, trainable=True)(input1)\n",
    "else:\n",
    "    # word2vec was loaded, load weights and set to untrainable\n",
    "    embedding = keras.layers.Embedding(num_words+1, embed_dim, weights=[embedding_matrix], trainable=False)(input1)\n",
    " \n",
    "# conv layers\n",
    "convs = []\n",
    "filter_sizes = [2,3,4]\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = keras.layers.Conv1D(filters=100,kernel_size=fsz,activation='relu')(embedding)\n",
    "    l_pool = keras.layers.MaxPooling1D(sequencelength-100+1)(l_conv)\n",
    "    l_pool = keras.layers.Flatten()(l_pool)\n",
    "    convs.append(l_pool)\n",
    "# merge conv layers\n",
    "l_merge = keras.layers.concatenate(convs, axis=1)\n",
    "# drop out regulation\n",
    "l_out = keras.layers.Dropout(0.5)(l_merge)\n",
    "# output layer\n",
    "output = keras.layers.Dense(units=9, activation='softmax')(l_out)\n",
    "# model\n",
    "model = keras.models.Model(input1, output)\n",
    "# compile model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['categorical_crossentropy'])\n",
    "# train model\n",
    "model.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, class_weight=class_weight, callbacks=[ckpt_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "test = pd.read_csv('stage2_test_variants.csv')\n",
    "# load test text dataset\n",
    "test_txt_ = pd.read_csv('stage2_test_text.csv', sep='\\|\\|', engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "# merge text & variants\n",
    "test = pd.merge(test, test_txt_, how='left', on='ID')\n",
    "# clean up\n",
    "del test_txt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expand records to sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEK2</td>\n",
       "      <td>1</td>\n",
       "      <td>The incidence of breast cancer is increasing i...</td>\n",
       "      <td>H371Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEK2</td>\n",
       "      <td>1</td>\n",
       "      <td>These preliminary studies suggest that, on acc...</td>\n",
       "      <td>H371Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEK2</td>\n",
       "      <td>1</td>\n",
       "      <td>Peripheral blood samples were collected from t...</td>\n",
       "      <td>H371Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEK2</td>\n",
       "      <td>1</td>\n",
       "      <td>In total, we detected six germline sequence al...</td>\n",
       "      <td>H371Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEK2</td>\n",
       "      <td>1</td>\n",
       "      <td>Bioinformatic analysis suggested that the p.H3...</td>\n",
       "      <td>H371Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gene  ID                                               Text Variation\n",
       "0  CHEK2   1  The incidence of breast cancer is increasing i...     H371Y\n",
       "1  CHEK2   1  These preliminary studies suggest that, on acc...     H371Y\n",
       "2  CHEK2   1  Peripheral blood samples were collected from t...     H371Y\n",
       "3  CHEK2   1  In total, we detected six germline sequence al...     H371Y\n",
       "4  CHEK2   1  Bioinformatic analysis suggested that the p.H3...     H371Y"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expanded from 986 to 29967\n"
     ]
    }
   ],
   "source": [
    "print('Expand records to sentences.')\n",
    "# temp dict for new train set\n",
    "tmpdf_ = {'Text': [], 'ID': [], 'Gene': [], 'Variation': []}\n",
    "for index, row in test.iterrows():\n",
    "    # get sentences nltk\n",
    "    sent_tokenize_list = nltk.sent_tokenize(row['Text'])\n",
    "    # truncate sentences to last maxnumberofsentences (most important informations are at the end of text)\n",
    "    if (len(sent_tokenize_list) > maxnumberofsentences):\n",
    "        sent_tokenize_list = sent_tokenize_list[len(sent_tokenize_list)-maxnumberofsentences:]\n",
    "    # split sentences to batch\n",
    "    sent_chunk = list(chunks(sent_tokenize_list, splitbysentences))\n",
    "    for chunk in sent_chunk:\n",
    "        # join sentences in text\n",
    "        tmpdf_['Text'].append(\" \".join(chunk))\n",
    "        # assign ID\n",
    "        tmpdf_['ID'].append(row['ID'])\n",
    "        # assign Gene\n",
    "        tmpdf_['Gene'].append(row['Gene'])\n",
    "        # assign Variation\n",
    "        tmpdf_['Variation'].append(row['Variation'])\n",
    "# create new train set from temp dict\n",
    "origtestlen = len(test)\n",
    "test = pd.DataFrame(tmpdf_)\n",
    "# clean up\n",
    "del tmpdf_\n",
    "# display head\n",
    "display(test.head())\n",
    "# display \n",
    "print('expanded from {} to {}'.format(origtestlen,len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29967/29967 [==============================] - 317s 11ms/step\n",
      "\n",
      "----------------------\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#  load best model\n",
    "model = keras.models.load_model(model_filename)\n",
    "# get sequences\n",
    "Xtest = tokenizer.texts_to_sequences(test['Text'])\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(Xtest, maxlen=sequencelength)\n",
    "# predict\n",
    "probas = model.predict(Xtest, verbose=1)\n",
    "# prepare data for submission\n",
    "submission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])\n",
    "# insert IDs\n",
    "submission_df.insert(loc=0, column='ID', value=test['ID'].values)\n",
    "# average grouped data\n",
    "submission_df = submission_df.groupby(['ID'], as_index=False).mean()\n",
    "# save to csv\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "# debug\n",
    "print(\"\\n----------------------\\n\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
